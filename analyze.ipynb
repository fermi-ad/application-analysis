{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controls Application Analysis\n",
    "\n",
    "Here we dump metrics from all applications, console, java, etc. and collect them in a single data frame. This allows us to explore the data and discover insights that would be difficult to know otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Single Source of Truth\n",
    "\n",
    "We will keep all data in the `app_data.json` file. So, if you generate data that could be used for future analysis, add it to the `app_data` data frame and then write the data frame back to the `app_data.json` file.\n",
    "\n",
    "```python\n",
    "# Example data concatenation\n",
    "# app_data = pd.concat([app_data, line_data])\n",
    "app_data.to_json(\"app_data.json\", orient=\"records\", indent=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output to JSON\n",
    "\n",
    "If you uncomment this, it will output to `test.json`. Use this as a guide to update the `app_data.json` file. Check the changes with a print or the `test.json` file before committing changes to `app_data.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_data = pd.read_json('app_data.json')\n",
    "# app_data = app_data.merge(line_data, on=\"program\")\n",
    "# app_data.to_json('test.json', orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "To install these you can run `python3 -m pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "# Even though we don't use this, it has useful defaults, so let's keep it.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_data = pd.read_json('app_data.json')\n",
    "java_data = pd.read_json('java_data.json')\n",
    "\n",
    "# We will want to filter by whether or not an application is mapped (NaN)\n",
    "# this only applies for pas\n",
    "\n",
    "# Filter by exection_count > 0 for pas & sas\n",
    "\n",
    "# Define logical conditions to cut application from data\n",
    "# count -> lines of code\n",
    "# execution count -> number of times application is opened \n",
    "nonobsolete_condition = app_data[\"status\"] == \"active\"\n",
    "nonzero_lines_condition = app_data[\"count\"]>0\n",
    "nonzero_execution_count_condition = app_data[\"execution_count\"]>0\n",
    "mapped_condition = app_data[\"index_page\"].notnull()\n",
    "\n",
    "# Logical AND all the conditions\n",
    "# cut_condition = nonobsolete_condition*nonzero_lines_condition*nonzero_execution_count_condition*mapped_condition\n",
    "cut_condition = nonobsolete_condition*nonzero_lines_condition\n",
    "print(f'{np.where(cut_condition)[0].size} elements made the cut out of {np.asarray(app_data[\"count\"]).size}.')\n",
    "\n",
    "lines = np.asarray(app_data['count'])[np.where(cut_condition)]\n",
    "exect_count = np.asarray(app_data['execution_count'])[np.where(cut_condition)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting some stats\n",
    "pas = app_data.loc[app_data['type'] == 'pas']\n",
    "sas = app_data.loc[app_data['type'] == 'sas']\n",
    "uls = app_data.loc[app_data['type'] == 'uls']\n",
    "# Total number of active PAs and SAs\n",
    "actives = app_data.loc[app_data['status'] == 'active']\n",
    "active_pas = actives.loc[actives['type'] == 'pas']\n",
    "active_sas = actives.loc[actives['type'] == 'sas']\n",
    "# Total number of active ULs\n",
    "active_uls = actives.loc[actives['type'] == 'uls']\n",
    "\n",
    "# lines of code sum\n",
    "sum_pa = active_pas['count'].sum()\n",
    "sum_sa = active_sas['count'].sum()\n",
    "sum_ul = active_uls['count'].sum()\n",
    "\n",
    "keeper_count  = actives.groupby('keeper').agg(['count']).stack()\n",
    "mapped_pas = active_pas.loc[active_pas['index_page'].notnull()]\n",
    "\n",
    "# criticalitity counts\n",
    "core = app_data.loc[app_data['criticality'] == 'core']\n",
    "critical = app_data.loc[app_data['criticality'] == 'critical']\n",
    "specialized = app_data.loc[app_data['criticality'] == 'specialized']\n",
    "\n",
    "# All index page mappings\n",
    "map_list = active_pas['index_page'].apply(pd.Series).stack().unique()\n",
    "machine = []\n",
    "for app in map_list :\n",
    "    machine.append(app[0])\n",
    "unique_machines = pd.DataFrame(machine)\n",
    "unique_machines = unique_machines.value_counts()\n",
    "\n",
    "plt.hist(machine, 15, label=\"Mapping by Index Page\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historgram of Keepers\n",
    "keepers = actives[['keeper']].values\n",
    "\n",
    "plt.hist(keepers, 70, label=\"keeper occurances\")\n",
    "plt.legend()\n",
    "\n",
    "# locs, labels = plt.xticks()\n",
    "# plt.setp(labels, rotation=90)\n",
    "plt.tick_params(axis='x', which='both', bottom=False, labelbottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping \n",
    "map_count = active_pas['index_page'].str.len()\n",
    "map_count_sum = map_count.sum()\n",
    "active_pas['map_count'] = active_pas['index_page'].str.len()\n",
    "\n",
    "plt.hist(map_count, 90, label=\"Mapping Count\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply fit to data\n",
    "def func(x, a, b, c):\n",
    "    return a * np.exp(-1.0*(x/b)) + c # Exponential decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of execution count\n",
    "\n",
    "x = np.linspace(0, exect_count.size - 1, exect_count.size)\n",
    "nbins = 100\n",
    "\n",
    "n, bin_borders, _ = plt.hist(exect_count, nbins, label=\"Execution Count\")\n",
    "bin_centers = bin_borders[:-1] + np.diff(bin_borders) / 2\n",
    "p0 = [1, 1E2, 1]\n",
    "# popt, pcov = curve_fit(func, bin_centers, n, p0)\n",
    "# plt.plot(bin_centers, func(bin_centers, *popt), 'r-', label=\"Exponential Fit\")\n",
    "#plt.xlim(0,50000)\n",
    "plt.gca().set_yscale('log')\n",
    "cutoff = 5 # units of fit characteristic decay constant\n",
    "# plt.axvline(x = cutoff*popt[1], ymin = 0, ymax = 1, color='green', label = f\"Cutoff = {cutoff}\"+r\"$\\lambda$\")\n",
    "plt.legend()\n",
    "\n",
    "# print(f\"Decay constant is {popt[1]}\")\n",
    "\n",
    "# Compute total lines of code within the cutoff\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]\n",
    "# Find index that is nearest to the cutoff, integrate number of lines up to cutoff\n",
    "# Note from Adam: I don't think this is working yet.\n",
    "#cutoff_index = np.where(lines == find_nearest(lines, func(x,*popt)))[0][0]\n",
    "#lines_at_cutoff = np.sum(lines[0:cutoff_index])\n",
    "#print(f\"Total lines of code up to cutoff is {lines_at_cutoff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but for liines of code\n",
    "\n",
    "x = np.linspace(0, lines.size - 1, lines.size)\n",
    "nbins = 100\n",
    "\n",
    "n, bin_borders, _ = plt.hist(lines, nbins, label=\"Lines of Code\")\n",
    "bin_centers = bin_borders[:-1] + np.diff(bin_borders) / 2\n",
    "p0 = [1, 1E2, 1]\n",
    "# popt, pcov = curve_fit(func, bin_centers, n, p0)\n",
    "# plt.plot(bin_centers, func(bin_centers, *popt), 'r-', label=\"Exponential Fit\")\n",
    "#plt.xlim(0,50000)\n",
    "plt.gca().set_yscale('log')\n",
    "cutoff = 5 # units of fit characteristic decay constant\n",
    "# plt.axvline(x = cutoff*popt[1], ymin = 0, ymax = 1, color='green', label = f\"Cutoff = {cutoff}\"+r\"$\\lambda$\")\n",
    "plt.legend()\n",
    "\n",
    "# print(f\"Decay constant is {popt[1]}\")\n",
    "\n",
    "# Compute total lines of code within the cutoff\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]\n",
    "# Find index that is nearest to the cutoff, integrate number of lines up to cutoff\n",
    "# Note from Adam: I don't think this is working yet.\n",
    "#cutoff_index = np.where(lines == find_nearest(lines, func(x,*popt)))[0][0]\n",
    "#lines_at_cutoff = np.sum(lines[0:cutoff_index])\n",
    "#print(f\"Total lines of code up to cutoff is {lines_at_cutoff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some statistics to see what's interesting\n",
    "mean = np.mean(lines)\n",
    "median = np.median(lines)\n",
    "mode = stats.mode(lines)[0]\n",
    "sum = np.sum(lines)\n",
    "max = np.max(lines)\n",
    "print(f\"Mean: {mean:.0f}, Median: {median:.0f}, Mode: {mode:.0f}, Sum: {sum:.0f}, Max: {max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine PA# nearest to median\n",
    "median_index = np.where(lines == find_nearest(lines, np.median(lines)))[0][0]\n",
    "app_data[\"program\"][median_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideas:\n",
    "- Remove unused applications from dataset, if possible (i.e. not mapped to index page, not frequently used over time frame, Linac)\n",
    "- Weight applications by priority (usage stats and interview criticality), redo analysis to determine window\n",
    "- Use exponential fit, choose windows as integer multiples fit characteristic time constant (instead of standard deviation)\n",
    "\n",
    "Maybe for CD-1, do first and third idea above. Priority weighting can refine estimate for CD-2.\n",
    "\n",
    "John: effort is not linear with lines-of-code. Really wants to see how long it takes to port median application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation Calculations\n",
    "\n",
    "Functions below include assumptions and equations for quickly calculating estimates based on lines of code (loc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_page_loc = app_data[app_data[\"program\"] == \"pa0052\"][\"count\"] # pa0052 is the parameter page id\n",
    "# The parameter page took .25 FTEs to reach minimal viable product (MVP)\n",
    "ftes_to_mvp = .25\n",
    "# Each 204.05 WBS has 5 FTEs available\n",
    "available_ftes = 5\n",
    "# An FTE is 52 weeks\n",
    "fte_weeks = 52\n",
    "# While our iterations are 4 weeks, we only get 3 weeks of work to allow a week for catch-up and education\n",
    "iteration_weeks = 4\n",
    "iteration_work_weeks = 3\n",
    "# Available FTEs per week\n",
    "ftes_per_week = available_ftes/fte_weeks\n",
    "# We have 3 weeks of work per iteration\n",
    "work_per_iteration = ftes_per_week*iteration_work_weeks\n",
    "\n",
    "def loc_to_ftes(loc):\n",
    "    return loc*ftes_to_mvp/parameter_page_loc\n",
    "\n",
    "def ftes_to_iterations(ftes):\n",
    "    return ftes/work_per_iteration\n",
    "\n",
    "def loc_to_iterations(loc):\n",
    "    return ftes_to_iterations(loc_to_ftes(loc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_data['ftes'] = app_data['count'].apply(loc_to_ftes)\n",
    "app_data['iterations'] = app_data['count'].apply(loc_to_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "`get_dependencies.py` generates `dependencies.json`.\n",
    "\n",
    "`dependencies.json` has the reported dependencies of each property by reading the the associated `depends.opt` in the project directory in mecca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data\n",
    "with open('dependencies.json', 'r') as fp:\n",
    "   data = json.load(fp)\n",
    "\n",
    "# Preprocess the JSON data\n",
    "preprocessed_data = []\n",
    "for project, deps in data.items():\n",
    "   for dep in deps:\n",
    "       preprocessed_data.append({'project': project, 'dependency': dep})\n",
    "\n",
    "# Load the preprocessed data into a DataFrame\n",
    "dependencies = pd.DataFrame(preprocessed_data)\n",
    "grouped_dependencies = dependencies.groupby('project')\n",
    "no_dependencies = grouped_dependencies.filter(lambda x: len(x) == 0)\n",
    "counts = dependencies['dependency'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively track dependencies to see if there are circular dependencies\n",
    "def find_cycle(project, visited, stack, path, dependencies):\n",
    "    # Mark the current node as visited and add it to the stack\n",
    "    visited.add(project)\n",
    "    stack.add(project)\n",
    "    path.append(project)\n",
    "\n",
    "    # Recur for all dependencies of this project\n",
    "    for dep in dependencies[dependencies['project'] == project]['dependency']:\n",
    "        # print(dep)\n",
    "        # If the dependency is a project, and is not visited, then recur on it\n",
    "        # if dep in dependencies['project'] and dep not in visited:\n",
    "        if dependencies['project'].str.contains(dep).any() and dep not in visited:\n",
    "            cycle = find_cycle(dep, visited, stack, path, dependencies)\n",
    "            if cycle:\n",
    "                return cycle\n",
    "        # If the dependency is in the stack, then it has a cycle\n",
    "        elif dep in stack:\n",
    "            # Get the index of the dependency in the path\n",
    "            index = path.index(dep)\n",
    "            # Return the cycle from the path\n",
    "            return path[index:]\n",
    "\n",
    "    # Remove the project from the stack as it finishes\n",
    "    stack.remove(project)\n",
    "    path.pop()\n",
    "\n",
    "    return None\n",
    "\n",
    "def find_interdependencies(dependencies):\n",
    "    # Create a set to store visited projects and a stack\n",
    "    visited = set()\n",
    "    stack = set()\n",
    "    path = []\n",
    "\n",
    "    # Call the recursive function to detect cycle in different DFS trees\n",
    "    for project in dependencies['project']:\n",
    "        if project not in visited:\n",
    "            cycle = find_cycle(project, visited, stack, path, dependencies)\n",
    "            if cycle:\n",
    "                return cycle\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_interdependencies(dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively find all dependencies of a project\n",
    "def find_dependencies(project, visited, dependencies):\n",
    "    # Mark the current node as visited and add it to the stack\n",
    "    visited.add(project)\n",
    "\n",
    "    # Recur for all dependencies of this project\n",
    "    for dep in dependencies[dependencies['project'] == project]['dependency']:\n",
    "        # If the dependency is a project, and is not visited, then recur on it\n",
    "        if dependencies['project'].str.contains(dep).any() and dep not in visited:\n",
    "            find_dependencies(dep, visited, dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_dependencies = set()\n",
    "find_dependencies('acld', found_dependencies, dependencies)\n",
    "print(found_dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beau = pd.read_json('app_data_scope_beau.json')\n",
    "guz = pd.read_json('app_data_scope_guzman.json')\n",
    "\n",
    "common_programs = beau['program'][beau['program'].isin(guz['program'])]\n",
    "uncommon_programs_beau = beau[~beau['program'].isin(guz['program'])]\n",
    "uncommon_programs_guzman = guz[~guz['program'].isin(beau['program'])]\n",
    "\n",
    "print(f'overlap: ${len(common_programs)}')\n",
    "print(f'guz unique: ${len(uncommon_programs_guzman)}')\n",
    "print(f'beau unique: ${len(uncommon_programs_beau)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_data[app_data['status'] == 'active'][~app_data['sqa_level'].isin(['unknown', 'Low', 'Medium'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_core = app_data[app_data['status'] == 'active'][app_data['criticality'] == 'core']\n",
    "active_critical = app_data[app_data['status'] == 'active'][app_data['criticality'] == 'critical']\n",
    "active_specialized = app_data[app_data['status'] == 'active'][app_data['criticality'] == 'specialized']\n",
    "\n",
    "# sum the iterations\n",
    "sum_iterations = app_data['iterations'].sum()\n",
    "sum_core_iterations = active_core['iterations'].sum()\n",
    "sum_critical_iterations = active_critical['iterations'].sum()\n",
    "sum_specialized_iterations = active_specialized['iterations'].sum()\n",
    "\n",
    "print(sum_iterations)\n",
    "print(sum_core_iterations)\n",
    "print(sum_critical_iterations)\n",
    "print(sum_specialized_iterations)\n",
    "print(len(active_critical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish cuts for CD-1\n",
    "\n",
    "This is the strategy for determining which applications and support services are included in the project effort through FY29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "active_with_loc_cut = nonobsolete_condition*nonzero_lines_condition\n",
    "aggressive_cut = nonobsolete_condition*nonzero_lines_condition*nonzero_execution_count_condition*mapped_condition\n",
    "aggressive_cut2 = nonobsolete_condition*nonzero_lines_condition*nonzero_execution_count_condition\n",
    "# test_pages = app_data['index_page'].str.contains('W|X|Z')\n",
    "# non_test_pages = ~app_data['index_page'].str.contains('[A-Za-z]', na=False)\n",
    "# test_index_page_cut = aggressive_cut & (app_data['index_page'].str.contains('W|X|Z')) & (~app_data['index_page'].str.contains('[A-Za-z]', na=False))\n",
    "# non_test_cut_test = app_data['index_page'].str.contains('W')\n",
    "# test_pages = app_data['index_page'].map(lambda x: 'W' in x or 'X' in x or 'Z' in x)\n",
    "# test_pages = app_data['index_page'].str.contains('[WXZ]', na=True)\n",
    "# non_test_pages = app_data['index_page'].str.contains('[A-Z]', na=True)\n",
    "# non_test_pages = app_data['index_page'].map(lambda x: bool(re.match('[A-Z]', x)))\n",
    "# non_test_cut_test = app_data['index_page'].dropna().str.contains('W|X|Z')\n",
    "# non_test_cut_non = ~app_data['index_page'].str.contains('[A-Za-z]', na=False)\n",
    "# non_test_cut = test_pages & (~app_data['index_page'].str.contains('[A-Za-z]', na=False))\n",
    "# test_index_page_cut = test_pages & ~non_test_pages\n",
    "print(f'{np.where(cut_condition)[0].size} elements made the cut out of {np.asarray(app_data[\"count\"]).size}.')\n",
    "\n",
    "# active_with_loc = app_data[active_with_loc_cut]\n",
    "# aggressive = app_data[aggressive_cut]\n",
    "# non_test = app_data[test_index_page_cut]\n",
    "# print(aggressive)\n",
    "# print(test_index_page_cut)\n",
    "\n",
    "def is_test_page(value):\n",
    "    if value is None:\n",
    "        return False\n",
    "    else:\n",
    "        for page in value:\n",
    "            if re.match('[A-VY]\\d{1,3}', page):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "non_test_pages = app_data['index_page'].transform(is_test_page)\n",
    "blah = app_data[non_test_pages]\n",
    "print(blah['iterations'].sum())\n",
    "# print(non_test_cut_non)\n",
    "# print(non_test_cut)\n",
    "# print(aggressive_cut2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_data[\"line_usage\"] = app_data[\"execution_count\"]/app_data[\"count\"]\n",
    "app_data[\"usage_rank\"] = app_data[\"execution_count\"].rank(method='dense', ascending=False)\n",
    "app_data[\"line_rank\"] = app_data[\"count\"].rank(method='dense', ascending=False)\n",
    "app_data[\"line_usage_rank\"] = app_data[\"line_usage\"].rank(method='dense', ascending=False)\n",
    "app_data[\"line_and_usage_rank\"] = app_data[\"usage_rank\"] + app_data[\"line_rank\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_data[\"count\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_data[app_data[\"status\"] == \"active\"][\"count\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import NaN\n",
    "\n",
    "naive = app_data[app_data[\"status\"] == \"active\"][\"iterations\"].sum()/2/12\n",
    "\n",
    "manual_inclusion = [\n",
    "    \"ul_acl\",\n",
    "    \"pa0001\", # Index page\n",
    "    \"ul_dabbel\",\n",
    "    \"ul_acld_protocol\",\n",
    "    \"ul_drf2\",\n",
    "    \"ul_scopes\",\n",
    "    \"ul_clib\",\n",
    "    \"ul_bpmtbt\",\n",
    "    \"ul_cbsaux\",\n",
    "    \"ul_mecarlib\",\n",
    "    \"ul_model\",\n",
    "    \"ul_multiwire\",\n",
    "    \"ul_nova\",\n",
    "    \"ul_pa1516\",\n",
    "    \"ul_pasa\",\n",
    "    \"ul_pccns\",\n",
    "    \"ul_physlib\",\n",
    "    \"ul_calc_device\",\n",
    "    \"ul_lma\",\n",
    "    \"ul_lssol\",\n",
    "    \"ul_vacuum\",\n",
    "    \"ul_waller\",\n",
    "    \"pa4240\", # This is a service to launch registered ACL scripts.\n",
    "    \"pa2041\", # This program provides support services to the Sequencer.\n",
    "    \"pa0095\", # Curve Fit II program which fits data to exps or polynomials\n",
    "    \"pa4138\", # This program provides various ACL support utilities.\n",
    "    \"pa1881\", # This application launches ACL scripts\n",
    "    \"pa1535\", # This is a tool for Fourier signal analysis.\n",
    "    \"pa1661\", # Multicast alarms. (The alarm screen)\n",
    "    \"pa4234\", # Downtime logger\n",
    "]\n",
    "audited_lines_of_code = {\n",
    "    \"ul_acl\": 744_228,\n",
    "    \"ul_clib\": 200_000,\n",
    "    \"pa4283\": 2_069,\n",
    "    \"ul_cbsaux\": 269_084,\n",
    "    \"pa1048\": 76_521,\n",
    "    \"pa4247\": 0,\n",
    "    \"pa1860\": 0,\n",
    "}\n",
    "\n",
    "for inclusion in manual_inclusion:\n",
    "    app_data.loc[app_data[\"program\"] == inclusion, \"criticality\"] = \"manual\"\n",
    "\n",
    "# Copy count column to new \"audited_loc\" column\n",
    "app_data[\"audited_loc\"] = app_data.loc[:, \"count\"]\n",
    "\n",
    "# TODO: Manually adjust LOC\n",
    "for loc in audited_lines_of_code:\n",
    "    app_data.loc[app_data[\"program\"] == loc, \"audited_loc\"] = audited_lines_of_code[loc]\n",
    "\n",
    "app_data[\"audited_loc_ftes\"] = app_data[\"audited_loc\"].apply(loc_to_ftes)\n",
    "\n",
    "apps_to_modernize = app_data[app_data[\"status\"] == \"active\"][app_data[\"criticality\"].notna()]\n",
    "not_to_modernize = app_data[app_data[\"status\"] == \"active\"][~app_data[\"criticality\"].notna()]\n",
    "not_to_modernize.to_csv(\"not_to_modernize.csv\")\n",
    "apps_to_modernize[\"audited_loc_ftes\"].sum()\n",
    "# app_data[app_data[\"criticality\"] == \"specialized\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = app_data[app_data[\"status\"] == \"active\"][app_data[\"description\"].str.contains(\"plot\", case=False, na=False)]\n",
    "plots_filtered = plots[~plots[\"description\"].str.contains(\"vac|loss|blm|multiwire|tlg|cdf|downtime|tev|waterfall|tune|tbt|mwire|torpedo|example|damper|oddmod|bpm|scope|beam|bs events|lecroy|timeline|histogram|MiniBoone|wirescan|oxygen\", case=False, na=False)]\n",
    "iterations = plots_filtered[\"iterations\"].sum()\n",
    "\n",
    "print(f'{iterations}')\n",
    "print(f'{iterations*5}')\n",
    "print(plots_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_critical.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_apps = app_data[app_data[\"criticality\"] == \"core\"][\"description\"]\n",
    "critical_apps = app_data[app_data[\"criticality\"] == \"critical\"][\"description\"]\n",
    "specialized_apps = app_data[app_data[\"criticality\"] == \"specialized\"][\"description\"]\n",
    "core_apps.to_json('core_apps.json', orient='records', indent=2)\n",
    "critical_apps.to_json('critical_apps.json', orient='records', indent=2)\n",
    "specialized_apps.to_json('specialized_apps.json', orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
