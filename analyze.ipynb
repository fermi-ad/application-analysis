{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controls Application Analysis\n",
    "\n",
    "Here we dump metrics from all applications, console, java, etc. and collect them in a single data frame. This allows us to explore the data and discover insights that would be difficult to know otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Single Source of Truth\n",
    "\n",
    "We will keep all data in the `app_data.json` file. So, if you generate data that could be used for future analysis, add it to the `app_data` data frame and then write the data frame back to the `app_data.json` file.\n",
    "\n",
    "```python\n",
    "# Example data concatenation\n",
    "# app_data = pd.concat([app_data, line_data])\n",
    "app_data.to_json(\"app_data.json\", orient=\"records\", indent=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output to JSON\n",
    "\n",
    "If you uncomment this, it will output to `test.json`. Use this as a guide to update the `app_data.json` file. Check the changes with a print or the `test.json` file before committing changes to `app_data.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_data = pd.read_json('app_data.json')\n",
    "# app_data = app_data.merge(line_data, on=\"program\")\n",
    "# app_data.to_json('test.json', orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "To install these you can run `python3 -m pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "# Even though we don't use this, it has useful defaults, so let's keep it.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_data = pd.read_json('app_data.json')\n",
    "java_data = pd.read_json('java_data.json')\n",
    "\n",
    "# We will want to filter by whether or not an application is mapped (NaN)\n",
    "# this only applies for pas\n",
    "\n",
    "# Filter by exection_count > 0 for pas & sas\n",
    "\n",
    "# Define logical conditions to cut application from data\n",
    "# count -> lines of code\n",
    "# execution count -> number of times application is opened \n",
    "nonobsolete_condition = app_data[\"status\"] == \"active\"\n",
    "nonzero_lines_condition = app_data[\"count\"]>0\n",
    "nonzero_execution_count_condition = app_data[\"execution_count\"]>0\n",
    "mapped_condition = app_data[\"index_page\"].notnull()\n",
    "\n",
    "# Logical AND all the conditions\n",
    "# cut_condition = nonobsolete_condition*nonzero_lines_condition*nonzero_execution_count_condition*mapped_condition\n",
    "cut_condition = nonobsolete_condition*nonzero_lines_condition\n",
    "print(f'{np.where(cut_condition)[0].size} elements made the cut out of {np.asarray(app_data[\"count\"]).size}.')\n",
    "\n",
    "lines = np.asarray(app_data['count'])[np.where(cut_condition)]\n",
    "exect_count = np.asarray(app_data['execution_count'])[np.where(cut_condition)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting some stats\n",
    "pas = app_data.loc[app_data['type'] == 'pas']\n",
    "sas = app_data.loc[app_data['type'] == 'sas']\n",
    "uls = app_data.loc[app_data['type'] == 'uls']\n",
    "# Total number of active PAs and SAs\n",
    "actives = app_data.loc[app_data['status'] == 'active']\n",
    "active_pas = actives.loc[actives['type'] == 'pas']\n",
    "active_sas = actives.loc[actives['type'] == 'sas']\n",
    "# Total number of active ULs\n",
    "active_uls = actives.loc[actives['type'] == 'uls']\n",
    "\n",
    "# lines of code sum\n",
    "sum_pa = active_pas['count'].sum()\n",
    "sum_sa = active_sas['count'].sum()\n",
    "sum_ul = active_uls['count'].sum()\n",
    "\n",
    "keeper_count  = actives.groupby('keeper').agg(['count']).stack()\n",
    "mapped_pas = active_pas.loc[active_pas['index_page'].notnull()]\n",
    "\n",
    "# criticalitity counts\n",
    "core = app_data.loc[app_data['criticality'] == 'core']\n",
    "critical = app_data.loc[app_data['criticality'] == 'critical']\n",
    "specialized = app_data.loc[app_data['criticality'] == 'specialized']\n",
    "\n",
    "# All index page mappings\n",
    "map_list = active_pas['index_page'].apply(pd.Series).stack().unique()\n",
    "machine = []\n",
    "for app in map_list :\n",
    "    machine.append(app[0])\n",
    "unique_machines = pd.DataFrame(machine)\n",
    "unique_machines = unique_machines.value_counts()\n",
    "\n",
    "plt.hist(machine, 15, label=\"Mapping by Index Page\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historgram of Keepers\n",
    "keepers = actives[['keeper']].values\n",
    "\n",
    "plt.hist(keepers, 70, label=\"keeper occurances\")\n",
    "plt.legend()\n",
    "\n",
    "# locs, labels = plt.xticks()\n",
    "# plt.setp(labels, rotation=90)\n",
    "plt.tick_params(axis='x', which='both', bottom=False, labelbottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping \n",
    "map_count = active_pas['index_page'].str.len()\n",
    "map_count_sum = map_count.sum()\n",
    "active_pas['map_count'] = active_pas['index_page'].str.len()\n",
    "\n",
    "plt.hist(map_count, 90, label=\"Mapping Count\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply fit to data\n",
    "def func(x, a, b, c):\n",
    "    return a * np.exp(-1.0*(x/b)) + c # Exponential decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of execution count\n",
    "\n",
    "x = np.linspace(0, exect_count.size - 1, exect_count.size)\n",
    "nbins = 100\n",
    "\n",
    "n, bin_borders, _ = plt.hist(exect_count, nbins, label=\"Execution Count\")\n",
    "bin_centers = bin_borders[:-1] + np.diff(bin_borders) / 2\n",
    "p0 = [1, 1E2, 1]\n",
    "# popt, pcov = curve_fit(func, bin_centers, n, p0)\n",
    "# plt.plot(bin_centers, func(bin_centers, *popt), 'r-', label=\"Exponential Fit\")\n",
    "#plt.xlim(0,50000)\n",
    "plt.gca().set_yscale('log')\n",
    "cutoff = 5 # units of fit characteristic decay constant\n",
    "# plt.axvline(x = cutoff*popt[1], ymin = 0, ymax = 1, color='green', label = f\"Cutoff = {cutoff}\"+r\"$\\lambda$\")\n",
    "plt.legend()\n",
    "\n",
    "# print(f\"Decay constant is {popt[1]}\")\n",
    "\n",
    "# Compute total lines of code within the cutoff\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]\n",
    "# Find index that is nearest to the cutoff, integrate number of lines up to cutoff\n",
    "# Note from Adam: I don't think this is working yet.\n",
    "#cutoff_index = np.where(lines == find_nearest(lines, func(x,*popt)))[0][0]\n",
    "#lines_at_cutoff = np.sum(lines[0:cutoff_index])\n",
    "#print(f\"Total lines of code up to cutoff is {lines_at_cutoff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but for liines of code\n",
    "\n",
    "x = np.linspace(0, lines.size - 1, lines.size)\n",
    "nbins = 100\n",
    "\n",
    "n, bin_borders, _ = plt.hist(lines, nbins, label=\"Lines of Code\")\n",
    "bin_centers = bin_borders[:-1] + np.diff(bin_borders) / 2\n",
    "p0 = [1, 1E2, 1]\n",
    "# popt, pcov = curve_fit(func, bin_centers, n, p0)\n",
    "# plt.plot(bin_centers, func(bin_centers, *popt), 'r-', label=\"Exponential Fit\")\n",
    "#plt.xlim(0,50000)\n",
    "plt.gca().set_yscale('log')\n",
    "cutoff = 5 # units of fit characteristic decay constant\n",
    "# plt.axvline(x = cutoff*popt[1], ymin = 0, ymax = 1, color='green', label = f\"Cutoff = {cutoff}\"+r\"$\\lambda$\")\n",
    "plt.legend()\n",
    "\n",
    "# print(f\"Decay constant is {popt[1]}\")\n",
    "\n",
    "# Compute total lines of code within the cutoff\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]\n",
    "# Find index that is nearest to the cutoff, integrate number of lines up to cutoff\n",
    "# Note from Adam: I don't think this is working yet.\n",
    "#cutoff_index = np.where(lines == find_nearest(lines, func(x,*popt)))[0][0]\n",
    "#lines_at_cutoff = np.sum(lines[0:cutoff_index])\n",
    "#print(f\"Total lines of code up to cutoff is {lines_at_cutoff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some statistics to see what's interesting\n",
    "mean = np.mean(lines)\n",
    "median = np.median(lines)\n",
    "mode = stats.mode(lines)[0]\n",
    "sum = np.sum(lines)\n",
    "max = np.max(lines)\n",
    "print(f\"Mean: {mean:.0f}, Median: {median:.0f}, Mode: {mode:.0f}, Sum: {sum:.0f}, Max: {max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine PA# nearest to median\n",
    "median_index = np.where(lines == find_nearest(lines, np.median(lines)))[0][0]\n",
    "app_data[\"program\"][median_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideas:\n",
    "- Remove unused applications from dataset, if possible (i.e. not mapped to index page, not frequently used over time frame, Linac)\n",
    "- Weight applications by priority (usage stats and interview criticality), redo analysis to determine window\n",
    "- Use exponential fit, choose windows as integer multiples fit characteristic time constant (instead of standard deviation)\n",
    "\n",
    "Maybe for CD-1, do first and third idea above. Priority weighting can refine estimate for CD-2.\n",
    "\n",
    "John: effort is not linear with lines-of-code. Really wants to see how long it takes to port median application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation Calculations\n",
    "\n",
    "Functions below include assumptions and equations for quickly calculating estimates based on lines of code (loc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_page_loc = app_data[app_data[\"program\"] == \"pa0052\"][\"count\"] # pa0052 is the parameter page id\n",
    "# The parameter page took .25 FTEs to reach minimal viable product (MVP)\n",
    "ftes_to_mvp = .25\n",
    "# Each 204.05 WBS has 5 FTEs available\n",
    "available_ftes = 5\n",
    "# An FTE is 52 weeks\n",
    "fte_weeks = 52\n",
    "# While our iterations are 4 weeks, we only get 3 weeks of work to allow a week for catch-up and education\n",
    "iteration_weeks = 4\n",
    "iteration_work_weeks = 3\n",
    "# Available FTEs per week\n",
    "ftes_per_week = available_ftes/fte_weeks\n",
    "# We have 3 weeks of work per iteration\n",
    "work_per_iteration = ftes_per_week*iteration_work_weeks\n",
    "\n",
    "def loc_to_ftes(loc):\n",
    "    return loc*ftes_to_mvp/parameter_page_loc\n",
    "\n",
    "def ftes_to_iterations(ftes):\n",
    "    return ftes/work_per_iteration\n",
    "\n",
    "def loc_to_iterations(loc):\n",
    "    return ftes_to_iterations(loc_to_ftes(loc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_data['ftes'] = app_data['count'].apply(loc_to_ftes)\n",
    "app_data['iterations'] = app_data['count'].apply(loc_to_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "`get_dependencies.py` generates `dependencies.json`.\n",
    "\n",
    "`dependencies.json` has the reported dependencies of each property by reading the the associated `depends.opt` in the project directory in mecca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data\n",
    "with open('dependencies.json', 'r') as fp:\n",
    "   data = json.load(fp)\n",
    "\n",
    "# Preprocess the JSON data\n",
    "preprocessed_data = []\n",
    "for project, deps in data.items():\n",
    "   for dep in deps:\n",
    "       preprocessed_data.append({'project': project, 'dependency': dep})\n",
    "\n",
    "# Load the preprocessed data into a DataFrame\n",
    "dependencies = pd.DataFrame(preprocessed_data)\n",
    "grouped_dependencies = dependencies.groupby('project')\n",
    "no_dependencies = grouped_dependencies.filter(lambda x: len(x) == 0)\n",
    "counts = dependencies['dependency'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively track dependencies to see if there are circular dependencies\n",
    "def find_cycle(project, visited, stack, path, dependencies):\n",
    "    # Mark the current node as visited and add it to the stack\n",
    "    visited.add(project)\n",
    "    stack.add(project)\n",
    "    path.append(project)\n",
    "\n",
    "    # Recur for all dependencies of this project\n",
    "    for dep in dependencies[dependencies['project'] == project]['dependency']:\n",
    "        # print(dep)\n",
    "        # If the dependency is a project, and is not visited, then recur on it\n",
    "        # if dep in dependencies['project'] and dep not in visited:\n",
    "        if dependencies['project'].str.contains(dep).any() and dep not in visited:\n",
    "            cycle = find_cycle(dep, visited, stack, path, dependencies)\n",
    "            if cycle:\n",
    "                return cycle\n",
    "        # If the dependency is in the stack, then it has a cycle\n",
    "        elif dep in stack:\n",
    "            # Get the index of the dependency in the path\n",
    "            index = path.index(dep)\n",
    "            # Return the cycle from the path\n",
    "            return path[index:]\n",
    "\n",
    "    # Remove the project from the stack as it finishes\n",
    "    stack.remove(project)\n",
    "    path.pop()\n",
    "\n",
    "    return None\n",
    "\n",
    "def find_interdependencies(dependencies):\n",
    "    # Create a set to store visited projects and a stack\n",
    "    visited = set()\n",
    "    stack = set()\n",
    "    path = []\n",
    "\n",
    "    # Call the recursive function to detect cycle in different DFS trees\n",
    "    for project in dependencies['project']:\n",
    "        if project not in visited:\n",
    "            cycle = find_cycle(project, visited, stack, path, dependencies)\n",
    "            if cycle:\n",
    "                return cycle\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_interdependencies(dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively find all dependencies of a project\n",
    "def find_dependencies(project, visited, dependencies):\n",
    "    # Mark the current node as visited and add it to the stack\n",
    "    visited.add(project)\n",
    "\n",
    "    # Recur for all dependencies of this project\n",
    "    for dep in dependencies[dependencies['project'] == project]['dependency']:\n",
    "        # If the dependency is a project, and is not visited, then recur on it\n",
    "        if dependencies['project'].str.contains(dep).any() and dep not in visited:\n",
    "            find_dependencies(dep, visited, dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_dependencies = set()\n",
    "find_dependencies('acld', found_dependencies, dependencies)\n",
    "print(found_dependencies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
